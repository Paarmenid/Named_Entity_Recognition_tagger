{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddinge skinuti s google drivea ispod te staviti u folder u kojemu se nalazi notebook.\n",
    "### Embeddinzi su izračunati u drugom notebooku.\n",
    "### Trening traje 4+ sati pa da se skrati proces.\n",
    "\n",
    "### https://drive.google.com/drive/folders/1YFfS220nDpc59b7SxNp2e4omSWnemtSx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRENING MODELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "Lnge-7-Q3zYy",
    "outputId": "f6e6c39a-c655-4d05-f940-f2bd1529e786",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan Emanuel Pavlov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ivan Emanuel Pavlov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21704 samples, validate on 4320 samples\n",
      "Epoch 1/3\n",
      "21704/21704 [==============================] - 284s 13ms/step - loss: 0.0637 - accuracy: 0.9852 - val_loss: 0.0473 - val_accuracy: 0.9896\n",
      "Epoch 2/3\n",
      "21704/21704 [==============================] - 269s 12ms/step - loss: 0.0433 - accuracy: 0.9904 - val_loss: 0.0430 - val_accuracy: 0.9910\n",
      "Epoch 3/3\n",
      "21704/21704 [==============================] - 269s 12ms/step - loss: 0.0391 - accuracy: 0.9916 - val_loss: 0.0356 - val_accuracy: 0.9925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DER       0.68      0.72      0.70        32\n",
      "         LOC       0.90      0.87      0.88       644\n",
      "         MIS       0.55      0.40      0.46       332\n",
      "         ORG       0.63      0.76      0.69       563\n",
      "         PER       0.87      0.82      0.84       660\n",
      "\n",
      "   micro avg       0.76      0.75      0.76      2231\n",
      "   macro avg       0.72      0.71      0.71      2231\n",
      "weighted avg       0.76      0.75      0.76      2231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import recall_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.layers.merge import add\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kreiraj_dataset():\n",
    "   \n",
    "   #Connllu file se skida s interneta te učitava tekst koji se potom obrađuje.\n",
    "   #Funkcija na kraju vraća dataframe s istim stupcima koji su u conllu fajlu\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/reldi-data/hr500k/master/hr500k.conllu\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('hr500k.conllu', 'wb').write(r.content)\n",
    "\n",
    "    tekst=[]\n",
    "    for tek in open('hr500k.conllu',\"r\",encoding=\"utf8\") .readlines():\n",
    "            if tek.startswith(\"#\") or tek.startswith(\"\\n\"):  \n",
    "              pass\n",
    "            else:\n",
    "              tekst.append(tek.split(\"\\t\"))  \n",
    "\n",
    "\n",
    "    tekst=pd.DataFrame(tekst)\n",
    "\n",
    "\n",
    "    tekst=tekst.drop([6,8],axis=1)\n",
    "    tekst=tekst.rename(columns={0:\"Word index\", 1:\"Riječ\",2:\"Korijen\",3:\"UPOS\",4:\"XPOS\",5:\"FEATS\",7:\"DEPREL\",9:\"Ostalo\"})\n",
    "    NER_tagovi=[]\n",
    "\n",
    "    for tag in tekst[\"Ostalo\"]:\n",
    "        ind=tag.find(\"NER=\")\n",
    "        if ind!=-1:\n",
    "          NER_tagovi.append(str.upper(tag[ind+4:ind+9]))\n",
    "        else:\n",
    "          NER_tagovi.append(\"O\")\n",
    "\n",
    "    tekst[\"NER_tag\"]=NER_tagovi\n",
    "    recenice=[]\n",
    "    broj_recenice=0\n",
    "    \n",
    "\n",
    "    for ind in tekst[\"Word index\"]:\n",
    "      if int(ind)==1:\n",
    "        broj_recenice+=1\n",
    "        recenice.append(broj_recenice)\n",
    "      else:\n",
    "        recenice.append(broj_recenice)\n",
    "        \n",
    "\n",
    "    tekst[\"Rečenica\"]=recenice\n",
    "\n",
    "    #tekst.to_csv('tekst.csv')\n",
    "\n",
    "    return tekst\n",
    "\n",
    "\n",
    "def predvidi_jedan(i):\n",
    "  #Ova funkcija uzima instancu iz test skupa te računa vjerojatnost da token pripada nekom NER tagu. Tada uzme NER tag za koji je predviđena najveća vjerojatnost i spremi tu vrijednost(broj), te isto tako za ostale riječi u rečenici.       \n",
    "  #U konačnici p je lista predviđenih NER tagova.\n",
    "  #y_true su stvarni NER tagovi.\n",
    "  #Pomoću riječnika indeksi_u_tagove (definiranog u kodu dolje) se dohvaćaju NER tagovi kao stringovi i list p koju funkcija vraća je lista NER stringova (za svaku riječ u rečenici).\n",
    "\n",
    "  p = model.predict(np.array(X_te[i:i+batch_size]))[0]\n",
    "  p = np.argmax(p, axis=-1)\n",
    "  p=list(p)\n",
    "    \n",
    "     \n",
    "  y_true=[]\n",
    "  for i in y_te[i]:\n",
    "   y_true.append(tags[i])\n",
    "\n",
    "  \n",
    "  for ind,y in enumerate(p):\n",
    "    tag=indeksi_u_tagove[y]\n",
    "    p[ind]=tag\n",
    "\n",
    "  return y_true,p\n",
    "\n",
    "def padding(X,padding):\n",
    "  #Funkcija koja dodaje ekstra tokene prekratkim rečenicama. To je potrebno jer za ulaz u NN trebamo rečenice jednakih duljina.\n",
    "\n",
    "  for i,x in enumerate(X):\n",
    "    padding_duljina=50-len(x)\n",
    "    \n",
    "    x[len(x):]=[padding for i in range(padding_duljina)]\n",
    "    #x.append(X.index[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Tu se kreira dataset te filtriraju stupci\n",
    "tekst=kreiraj_dataset()\n",
    "data=tekst[[\"Rečenica\",\"Riječ\",\"NER_tag\"]]\n",
    "\n",
    "\n",
    "\n",
    "#Liste sadrže sve riječi i tagove u korpusu\n",
    "words = list(set(data[\"Riječ\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "tags = list(set(data[\"NER_tag\"].values))\n",
    "n_tags = len(tags)\n",
    "\n",
    "\n",
    "\n",
    "#Riječnici koji pretvaraju stringove u brojeve\n",
    "max_len = 50\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "# tokeniziram riječi i NER_tagove\n",
    "\n",
    "\n",
    "\n",
    "#tokeni ovdje ustvari znače numerički token. Dakle ovo su stringovi pretvoreni u brojeve.\n",
    "word_tokeni=[]\n",
    "NER_tokeni=[]\n",
    "\n",
    "for word in data[\"Riječ\"]:\n",
    "  word_tokeni.append(word2idx[word])\n",
    "\n",
    "for tag in data[\"NER_tag\"]:\n",
    "  NER_tokeni.append(tag2idx[tag])\n",
    "\n",
    "data[\"Riječ_tok\"]=word_tokeni\n",
    "data[\"NER_tok\"]=NER_tokeni\n",
    "\n",
    "\n",
    "\n",
    "#Ovdje skupljam riječi iz rečenica u liste. Svaka rečenica je jedna lista. \n",
    "X_za_embedding=data.groupby(\"Rečenica\")[\"Riječ\"].apply(list)\n",
    "\n",
    "\n",
    "#tu trazim indekse recenica koje imaju vise od 75 rijeci jer se kasnije koristi input od 75 riječi za model. \n",
    "x_izbaciti=[]\n",
    "for i,x in enumerate(X_za_embedding):\n",
    "  if len(x)>=max_len:\n",
    "    \n",
    "    x_izbaciti.append(i+1)\n",
    "\n",
    "\n",
    "#Izbacujem rečenice s više od 75 riječi\n",
    "X_za_embedding=X_za_embedding.drop(index=x_izbaciti)\n",
    "\n",
    "\n",
    "#Isto kao i za X uz dodadno paddanje Y listi.\n",
    "Y=data.groupby(\"Rečenica\")[\"NER_tok\"].apply(list)\n",
    "\n",
    "y_izbaciti=[i-1 for i in x_izbaciti]\n",
    "Y=Y.drop(index=y_izbaciti)\n",
    "Y = pad_sequences(maxlen=max_len, sequences=Y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "  \n",
    "padding(X_za_embedding,\"ENDPAD\")\n",
    "\n",
    "X_za_embedding=X_za_embedding.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#Tu loadam embeddan korpus koji ide u  model\n",
    "with open('elmo.pkl', 'rb') as f:\n",
    "    ELMO = pickle.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "batch_size=32\n",
    "\n",
    "#train i test skup\n",
    "\n",
    "indeksi=list(X_za_embedding.index)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te,indeksi_tr,indeksi_te = train_test_split(ELMO, Y, indeksi, test_size=0.1, random_state=1000)\n",
    "X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]\n",
    "\n",
    "y_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]\n",
    "y_tr = np.array(y_tr).reshape(np.array(y_tr).shape[0], np.array(y_tr).shape[1], 1)\n",
    "y_val = np.array(y_val).reshape(np.array(y_val).shape[0], np.array(y_val).shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "#MODEL\n",
    "\n",
    "input_text = Input(shape=(max_len,1024),dtype=tf.float32)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                        dropout=0.2,recurrent_dropout=0.2))(input_text) \n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           dropout=0.2,recurrent_dropout=0.2))(x)\n",
    "x = add([x, x_rnn])\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(np.array(X_tr), np.array(y_tr), validation_data=(np.array(X_val), np.array(y_val)),\n",
    "                    batch_size=batch_size, epochs=3, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#računanje predviđanja za sve rečenice iz test skupa te spremanje u listu\n",
    "indeksi_u_tagove={list(tag2idx.values())[k]:str.upper(list(tag2idx.keys())[k]) for k in range(0,11)}\n",
    "\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for i in range(len(y_te)):\n",
    "  y1,y2=predvidi_jedan(i)\n",
    "  y_true.append(y1)\n",
    "  y_pred.append(y2)\n",
    "\n",
    "\n",
    "#bitne metrike koje govore o kvaliteti modela\n",
    "cl_report=classification_report(y_true, y_pred)\n",
    "for i in cl_report.split(\"\\n\"):\n",
    "  print(i)\n",
    "\n",
    "#f1 score je ispod 0.76, ja sam max dobio 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21704 samples, validate on 4320 samples\n",
      "Epoch 1/2\n",
      "21704/21704 [==============================] - 301s 14ms/step - loss: 0.0352 - accuracy: 0.9926 - val_loss: 0.0352 - val_accuracy: 0.9933\n",
      "Epoch 2/2\n",
      "21704/21704 [==============================] - 272s 13ms/step - loss: 0.0314 - accuracy: 0.9936 - val_loss: 0.0277 - val_accuracy: 0.9948\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DER       0.83      0.75      0.79        32\n",
      "         LOC       0.86      0.90      0.88       644\n",
      "         MIS       0.58      0.53      0.56       332\n",
      "         ORG       0.74      0.69      0.71       563\n",
      "         PER       0.86      0.86      0.86       660\n",
      "\n",
      "   micro avg       0.79      0.78      0.78      2231\n",
      "   macro avg       0.77      0.75      0.76      2231\n",
      "weighted avg       0.79      0.78      0.78      2231\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cF3uqMcctgQ"
   },
   "source": [
    "# Predict rečenice iz test skupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            True : Pred\n",
      "==============================\n",
      "O              :O     O\n",
      "životu         :O     O\n",
      "u              :O     O\n",
      "Hrvatskoj      :B-LOC B-LOC\n",
      ",              :O     O\n",
      "o              :O     O\n",
      "životu         :O     O\n",
      "u              :O     O\n",
      "Malaviju       :B-LOC B-LOC\n",
      ",              :O     O\n",
      "o              :O     O\n",
      "životu         :O     O\n",
      "u              :O     O\n",
      "Cape           :B-LOC B-LOC\n",
      "Maclearu       :I-LOC I-LOC\n",
      ".              :O     O\n"
     ]
    }
   ],
   "source": [
    "# biramo neku rečenicu te predictamo NER_tag vjerojatnost(znači ima ih 11, pa 11 rezultata) za neki token\n",
    "# s argmax biramo najvjerojatniji tag od tih 11, ovaj axis označava da biramo iz zadnje dimenzije arraya. Na kraju je 50 tagova, za 50 tokena (zbog paddinga)\n",
    "\n",
    "# s i biramo neku rečenicu, treba odabrati neki i koji ima u test setu\n",
    "\n",
    "\n",
    "i = 54\n",
    "p = model.predict(np.array(X_te[i:i+batch_size]))[0]\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} {:5}: {}\".format(\"Word\", \"True\",\"Pred\"))\n",
    "print(\"=\"*30)\n",
    "for w, true, pred in zip(X_za_embedding[indeksi_te[i]], y_te[i], p):\n",
    "    if w != \"ENDPAD\":\n",
    "        print(\"{:15}:{:5} {}\".format(w, tags[true],tags[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictanje novog teksta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 08:31:40,000 WARNING: Could not find config.  Trying C:\\Users\\Ivan Emanuel Pavlov\\OneDrive - Prirodoslovno-matematički fakultet\\Megatrend_zadatak\\ELMO-croatian\\cnn_50_100_512_4096_sample.json\n",
      "2021-12-03 08:31:40,001 WARNING: Could not find config.  Trying C:\\Users\\Ivan Emanuel Pavlov\\Anaconda3\\lib\\site-packages\\elmoformanylangs\\configs\\cnn_50_100_512_4096_sample.json\n",
      "2021-12-03 08:31:40,014 INFO: char embedding size: 2622\n",
      "2021-12-03 08:31:41,559 INFO: word embedding size: 299927\n",
      "2021-12-03 08:31:47,086 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(299927, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(2622, 50, padding_idx=2619)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#ovdje upisati svoj path gdje se nalazi elmo HR model\n",
    "path= r\"C:\\Users\\Ivan Emanuel Pavlov\\OneDrive - Prirodoslovno-matematički fakultet\\Megatrend_zadatak\\ELMO-croatian\"\n",
    "\n",
    "elmo_model = Embedder(path,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 08:31:48,026 INFO: 1 batches, avg len: 52.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Pred\n",
      "==============================\n",
      "Utah            B-ORG\n",
      "Jazz            I-ORG\n",
      "je              O\n",
      "stacionirana    O\n",
      "u               O\n",
      "Salt            B-LOC\n",
      "Lake            I-LOC\n",
      "City-u          I-LOC\n",
      "a               O\n",
      "najpoznatiji    O\n",
      "igrač           O\n",
      "im              O\n",
      "je              O\n",
      "Rudy            B-PER\n",
      "Gobert          I-PER\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tu se mogu umetnuti nove tokenizirane (već tokenizirane radi jednostavnosti, inače bi kao input naravno išle nove rečenice koja bi se potom tokenizirale kao ranije u kodu)\n",
    "#U listi moraju biti minimalno 2 rečenice\n",
    "\n",
    "test_sentence = [[\"Živim\",\"u\",\"Mozambiku\",\"te\",\"radim\",\"u\",\"Microsoftu\",\"a\",\"ime\",\"mi\",\"je\",\"Niko\",\"Takkoki\"],[\"Utah\",\"Jazz\",\"je\",\"stacionirana\",\"u\",\"Salt\",\"Lake\",\"City-u\",\"a\",\"najpoznatiji\",\"igrač\",\"im\",\"je\",\"Rudy\",\"Gobert\"]]\n",
    "    \n",
    "for rec in test_sentence:\n",
    "    for i in range(max_len-len(rec)):\n",
    "        rec.append(\"ENDPAD\")\n",
    "\n",
    "test_sentence_elmo=elmo_model.sents2elmo(np.array(test_sentence),output_layer=-1)\n",
    "\n",
    "\n",
    "\n",
    "#indeks recenice i\n",
    "i=1\n",
    "\n",
    "p = model.predict(np.array(test_sentence_elmo))\n",
    "p = np.argmax(p,axis=-1)\n",
    "\n",
    "\n",
    "print(\"{:15} {}\".format(\"Word\",\"Pred\"))\n",
    "print(\"=\"*30)\n",
    "for w, pred in zip(test_sentence[i], p[i]):\n",
    "    if w != \"ENDPAD\":\n",
    "        #print(pred)\n",
    "        print(\"{:15} {}\".format(w,tags[pred]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Izračun ELMO embeddinga za cijeli korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kod za izračun ELMO embeddinga za cijeli korpus\n",
    "\n",
    "\n",
    "#ovdje upisati svoj path gdje se nalazi elmo HR model\n",
    "# ELMO model se skida s https://github.com/HIT-SCIR/ELMoForManyLangs\n",
    "\n",
    "path= r\"C:\\Users\\Ivan Emanuel Pavlov\\OneDrive - Prirodoslovno-matematički fakultet\\Megatrend_zadatak\\ELMO-croatian\"\n",
    "\n",
    "\n",
    "#tu se učitava model koji je prethodno skinut s neta te se s njegovom funkcijom sents2elmo izračunavaju embeddingsi za sve rečenice u HR korpusu\n",
    "elmo_model = Embedder(path,batch_size=32)\n",
    "\n",
    "em=elmo_model.sents2elmo(np.array(list(X_za_embedding)),output_layer=-1)\n",
    "\n",
    "\n",
    "with open('elmo_2.pkl', 'wb') as f:\n",
    "    pickle.dump(em, f)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ELMO LSTM za NER.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
